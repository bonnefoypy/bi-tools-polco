#!/usr/bin/env python3
"""
POLCO ANALYZER 3.0 - Orchestrateur Principal
Syst√®me d'analyse retail avec architecture sectorielle
Coordonne les 4 processeurs sp√©cialis√©s + graphiques + assembleur final
"""

import os
import sys
from datetime import datetime
from typing import Dict, Any, Optional, List
import logging

# Configuration
PROJECT_ID = "polcoaigeneration-ved6"
DATA_COLLECTION = "polco_magasins_data"
RESULT_COLLECTION = "polco_analyzer_3_0"

# Configuration des logs
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('polco_analyzer.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class PolcoAnalyzer30:
    """Orchestrateur principal POLCO ANALYZER 3.0."""
    
    def __init__(self, captation_collection="polco_magasins_captation"):
        """Initialise l'orchestrateur."""
        self.project_id = PROJECT_ID
        self.data_collection = DATA_COLLECTION
        self.result_collection = RESULT_COLLECTION
        self.captation_collection = captation_collection
        self.db = None
        self.stats = {
            'total_stores': 0,
            'successful_analyses': 0,
            'failed_analyses': 0,
            'start_time': None,
            'processing_details': {},
            'errors': []
        }
        
        logger.info("üöÄ Initialisation POLCO ANALYZER 3.0")
    
    def check_credentials(self) -> bool:
        """V√©rifie les credentials."""
        if not os.path.exists("credentials.json"):
            logger.error("‚ùå Fichier credentials.json non trouv√©")
            return False
        
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.abspath("credentials.json")
        logger.info("‚úÖ Credentials configur√©s")
        return True
    
    def init_firestore(self) -> bool:
        """Initialise Firestore."""
        try:
            from google.cloud import firestore
            self.db = firestore.Client(project=self.project_id)
            logger.info("‚úÖ Firestore initialis√©")
            return True
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'initialisation de Firestore: {e}")
            return False
    
    def check_dependencies(self) -> bool:
        """V√©rifie que tous les modules sont disponibles."""
        try:
            # V√©rifier les processeurs sp√©cialis√©s
            from polco_contexte_processor import PolcoContexteProcessorV3
            from polco_cibles_processor import PolcoCiblesProcessorV3
            from polco_potentiel_processor import PolcoPotentielProcessorV3
            from polco_offre_processor import PolcoOffreProcessorV3
            from polco_actions_processor import PolcoActionsProcessorV3
            from polco_graphics_generator import PolcoGraphicsGenerator
            from polco_final_assembler import PolcoFinalAssembler
            
            logger.info("‚úÖ Tous les modules POLCO 3.0 v3 disponibles (5 processeurs + graphiques + assembleur)")
            return True
        except ImportError as e:
            logger.error(f"‚ùå Module manquant: {e}")
            return False
    
    def get_stores_data(self) -> List[Dict[str, Any]]:
        """R√©cup√®re les donn√©es des magasins depuis Firestore."""
        try:
            docs = self.db.collection(self.data_collection).stream()
            stores_data = []
            
            for doc in docs:
                store_data = doc.to_dict()
                stores_data.append(store_data)
            
            self.stats['total_stores'] = len(stores_data)
            logger.info(f"‚úÖ {self.stats['total_stores']} magasins r√©cup√©r√©s depuis {self.data_collection}")
            
            return stores_data
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration donn√©es magasins: {e}")
            return []
    
    def check_existing_analysis(self, store_id: str) -> Optional[Dict[str, Any]]:
        """V√©rifie si une analyse POLCO 3.0 existe d√©j√† pour ce magasin."""
        try:
            doc_ref = self.db.collection(self.result_collection).document(f"analyzer_3_0_{store_id}")
            doc = doc_ref.get()
            
            if doc.exists:
                existing_data = doc.to_dict()
                # V√©rifier que l'analyse est compl√®te (5 sections)
                sections = existing_data.get('sections_processed', [])
                if len(sections) >= 5:
                    logger.info(f"‚ôªÔ∏è [{store_id}] Analyse existante trouv√©e ({len(sections)} sections)")
                    return existing_data
                else:
                    logger.info(f"‚ö†Ô∏è [{store_id}] Analyse incompl√®te trouv√©e ({len(sections)}/5), reg√©n√©ration n√©cessaire")
                    return None
            else:
                logger.info(f"üÜï [{store_id}] Nouvelle analyse requise")
                return None
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [{store_id}] Erreur v√©rification existant: {e}")
            return None

    def detect_country_and_language(self, store_name: str, max_retries: int = 3) -> tuple:
        """D√©tecte le pays et la langue via LLM avec une logique de tentatives multiples."""
        import re
        import time
        try:
            # 1. Pr√©paration du prompt
            city_match = re.search(r'(?:DECATHLON\s+)?(.+?)(?:\s+\d+)?$', store_name.strip(), re.IGNORECASE)
            city_name = city_match.group(1).strip() if city_match else store_name.strip()
            
            detection_prompt = f"""Dans quel pays est situ√©e le decathlon de  "{city_name}", et quelle langue officielle principale y est utilis√©e?

R√©ponds UNIQUEMENT sous ce format exact:
PAYS: [Nom du pays en fran√ßais]
LANGUE: [Langue principale]

Exemples:
- Pour "Forbach": PAYS: France, LANGUE: Fran√ßais  
- Pour "M√ºnchen": PAYS: Allemagne, LANGUE: Deutsch
- Pour "Barcelona": PAYS: Espagne, LANGUE: Espa√±ol
- Pour "Milano": PAYS: Italie, LANGUE: Italiano"""
            logger.info(f"response: {detection_prompt}")
            # 2. Boucle de tentatives
            for attempt in range(max_retries):
                try:
                    # Initialiser le mod√®le si n√©cessaire
                    if not hasattr(self, 'model') or self.model is None:
                        from vertexai.generative_models import GenerativeModel
                        self.model = GenerativeModel("gemini-2.5-flash-lite")
                    
                    # Appel simple au mod√®le
                    response = self.model.generate_content(detection_prompt)
                    result = response.text.strip() if response.text else ""

                    logger.error(f"response: {result}")

                    # 3. Validation de la r√©ponse
                    if result:
                        country_match = re.search(r'PAYS:\s*([^,\n]+)', result, re.IGNORECASE)
                        language_match = re.search(r'LANGUE:\s*([^,\n]+)', result, re.IGNORECASE)
                        
                        if country_match and language_match:
                            country = country_match.group(1).strip()
                            language = language_match.group(1).strip()
                            logger.info(f"üåç D√©tection LLM r√©ussie: {city_name} -> {country}, {language}")
                            return country, language
                    
                    logger.warning(f"‚ö†Ô∏è R√©ponse invalide ou mal format√©e pour {store_name} (tentative {attempt + 1}/{max_retries})")

                except Exception as e:
                    logger.error(f"‚ùå Erreur d√©tection LLM pour {store_name} (tentative {attempt + 1}): {e}")
                    if attempt < max_retries - 1:
                        wait_time = (attempt + 1) * 5
                        logger.info(f"‚è≥ Attente {wait_time}s avant nouvelle tentative...")
                        time.sleep(wait_time)
            
            logger.error(f"‚ùå √âchec d√©finitif de la d√©tection pour {store_name} apr√®s {max_retries} tentatives.")

        except Exception as e:
            logger.error(f"‚ùå Erreur critique dans detect_country_and_language pour {store_name}: {e}")

        # 4. Fallback par d√©faut si toutes les tentatives √©chouent
        logger.info(f"üåç Fallback par d√©faut pour {store_name}: France, Fran√ßais")
        return 'France', 'Fran√ßais'

    def process_single_store(self, store_data: Dict[str, Any], force_regenerate: bool = False) -> Optional[Dict[str, Any]]:
        """Traite un magasin avec les 4 processeurs + graphiques + assemblage."""
        
        store_id = store_data.get('store_id', 'unknown')
        store_name = store_data.get('store_name', f'Store_{store_id}')
        force_regenerate = True
        try:
            # V√©rifier si l'analyse existe d√©j√† (sauf si force)
            if not force_regenerate:
                existing_analysis = self.check_existing_analysis(store_id)
                if existing_analysis:
                    logger.info(f"‚úÖ [{store_id}] Utilisation analyse existante")
                    return existing_analysis
            
            logger.info(f"üè™ [{store_id}] D√©marrage analyse compl√®te POLCO 3.0")
            start_time = datetime.now()
            
            # NOUVELLE LIGNE: D√©tection centralis√©e du pays et de la langue
            country, language = self.detect_country_and_language(store_name)
            
            # Importer les processeurs sp√©cialis√©s
            from polco_contexte_processor import PolcoContexteProcessorV3
            from polco_cibles_processor import PolcoCiblesProcessorV3
            from polco_potentiel_processor import PolcoPotentielProcessorV3
            from polco_offre_processor import PolcoOffreProcessorV3
            from polco_actions_processor import PolcoActionsProcessorV3
            from polco_graphics_generator import PolcoGraphicsGenerator
            from polco_final_assembler import PolcoFinalAssembler
            
            # Initialiser les processeurs v3 avec la collection de captation configur√©e
            contexte_processor = PolcoContexteProcessorV3(self.captation_collection)
            cibles_processor = PolcoCiblesProcessorV3(self.captation_collection)
            potentiel_processor = PolcoPotentielProcessorV3(self.captation_collection)
            offre_processor = PolcoOffreProcessorV3(self.captation_collection)
            actions_processor = PolcoActionsProcessorV3(self.captation_collection)
            graphics_generator = PolcoGraphicsGenerator()
            final_assembler = PolcoFinalAssembler()
            
            sections_results = []
            
            # 1. PROCESSEUR CONTEXTE v2
            logger.info(f"üéØ [{store_id}] Analyse CONTEXTE...")
            contexte_result = contexte_processor.process_store(store_data, country, language)
            if contexte_result:
                sections_results.append(contexte_result)
                logger.info(f"‚úÖ [{store_id}] Contexte termin√© ({contexte_result['metadata']['output_length']} chars)")
            else:
                logger.error(f"‚ùå [{store_id}] √âchec processeur CONTEXTE")
            
            # 2. PROCESSEUR CIBLES v2
            logger.info(f"üë• [{store_id}] Analyse CIBLES...")
            cibles_result = cibles_processor.process_store(store_data, country, language)
            if cibles_result:
                sections_results.append(cibles_result)
                logger.info(f"‚úÖ [{store_id}] Cibles termin√© ({cibles_result['metadata']['output_length']} chars)")
            else:
                logger.error(f"‚ùå [{store_id}] √âchec processeur CIBLES")
            
            # 3. PROCESSEUR POTENTIEL v2
            logger.info(f"üìà [{store_id}] Analyse POTENTIEL...")
            potentiel_result = potentiel_processor.process_store(store_data, country, language)
            if potentiel_result:
                sections_results.append(potentiel_result)
                logger.info(f"‚úÖ [{store_id}] Potentiel termin√© ({potentiel_result['metadata']['output_length']} chars)")
            else:
                logger.error(f"‚ùå [{store_id}] √âchec processeur POTENTIEL")
            
            # 4. PROCESSEUR OFFRE v3
            logger.info(f"üõçÔ∏è [{store_id}] Analyse OFFRE...")
            offre_result = offre_processor.process_store(store_data, country, language)
            if offre_result:
                sections_results.append(offre_result)
                logger.info(f"‚úÖ [{store_id}] Offre termin√© ({offre_result['metadata']['output_length']} chars)")
            else:
                logger.error(f"‚ùå [{store_id}] √âchec processeur OFFRE")
            
            # 5. PROCESSEUR ACTIONS v3 (Propositions d'actions bas√©es sur les 4 analyses)
            logger.info(f"üéØ [{store_id}] G√©n√©ration PROPOSITIONS D'ACTIONS...")
            actions_result = actions_processor.process_store(store_data, sections_results, country, language)
            if actions_result:
                sections_results.append(actions_result)
                logger.info(f"‚úÖ [{store_id}] Actions termin√© ({actions_result['metadata']['output_length']} chars)")
            else:
                logger.error(f"‚ùå [{store_id}] √âchec processeur ACTIONS")
            
            # V√©rifier qu'on a au moins une section
            if not sections_results:
                logger.error(f"‚ùå [{store_id}] Aucune section g√©n√©r√©e")
                return None
            
            # 6. G√âN√âRATEUR DE GRAPHIQUES
            logger.info(f"üìä [{store_id}] G√©n√©ration graphiques...")
            chart_filenames = graphics_generator.create_performance_dashboard(store_data, store_id)
            chart_integration = graphics_generator.generate_chart_markdown_integration(chart_filenames)
            
            if chart_filenames:
                logger.info(f"‚úÖ [{store_id}] {len(chart_filenames)} graphiques g√©n√©r√©s")
            else:
                logger.warning(f"‚ö†Ô∏è [{store_id}] Aucun graphique g√©n√©r√©")
            
            # 6. ASSEMBLEUR FINAL
            logger.info(f"üîß [{store_id}] Assemblage rapport final...")
            final_report = final_assembler.assemble_final_report(
                store_id, sections_results, store_data, chart_integration
            )
            
            # 7. SAUVEGARDE FIRESTORE
            logger.info(f"üíæ [{store_id}] Sauvegarde Firestore...")
            saved = final_assembler.save_final_report_to_firestore(
                final_report, self.db, self.result_collection
            )
            
            if saved:
                total_time = (datetime.now() - start_time).total_seconds()
                
                # Statistiques de traitement
                processing_stats = {
                    'sections_completed': len(sections_results),
                    'total_output_length': final_report['total_length'],
                    'charts_generated': len(chart_filenames),
                    'processing_time': total_time,
                    'sections_list': [s['section'] for s in sections_results]
                }
                
                self.stats['processing_details'][store_id] = processing_stats
                
                logger.info(f"üéâ [{store_id}] Analyse POLCO 3.0 termin√©e !")
                logger.info(f"üìä [{store_id}] Stats: {len(sections_results)} sections, "
                           f"{final_report['total_length']:,} chars, {total_time:.1f}s")
                
                return final_report
            else:
                logger.error(f"‚ùå [{store_id}] √âchec sauvegarde")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå [{store_id}] Erreur analyse compl√®te: {e}")
            self.stats['errors'].append(f"{store_id}: {str(e)}")
            return None
    
    def process_all_stores(self, limit: Optional[int] = None, target_store: Optional[str] = None) -> bool:
        """Traite tous les magasins avec POLCO 3.0."""
        
        # R√©cup√©rer les donn√©es
        stores_data = self.get_stores_data()
        
        if not stores_data:
            logger.error("‚ùå Aucune donn√©e magasin disponible")
            return False
        
        # Filtrer par store sp√©cifique si demand√©
        if target_store:
            stores_data = [s for s in stores_data if s.get('store_id') == target_store]
            if not stores_data:
                logger.error(f"‚ùå Store {target_store} non trouv√©")
                return False
            logger.info(f"üéØ Mode test: traitement du store {target_store} uniquement")
        # Limiter le nombre si demand√© (pour tests g√©n√©riques)
        elif limit:
            stores_data = stores_data[:limit]
            logger.info(f"üî¨ Mode test: traitement de {limit} magasins seulement")
        
        # Traiter chaque magasin
        for i, store_data in enumerate(stores_data, 1):
            store_id = store_data.get('store_id', f'store_{i}')
            
            logger.info(f"üè™ [{i}/{len(stores_data)}] Traitement magasin {store_id}")
            
            result = self.process_single_store(store_data, force_regenerate=getattr(self, 'force_regenerate', False))
            
            if result:
                self.stats['successful_analyses'] += 1
            else:
                self.stats['failed_analyses'] += 1
        
        return True
    
    def run(self, test_mode: bool = False, test_limit: int = 1, target_store: Optional[str] = None) -> bool:
        """Lance POLCO ANALYZER 3.0."""
        
        self.stats['start_time'] = datetime.now()
        
        logger.info("=" * 80)
        logger.info("üöÄ POLCO ANALYZER 3.0 - SYST√àME D'ANALYSE SECTORIELLE")
        logger.info("üìä Architecture: 4 Processeurs + Graphiques + Assembleur")
        logger.info("=" * 80)
        
        # V√©rifications
        if not self.check_credentials():
            return False
        
        if not self.init_firestore():
            return False
        
        if not self.check_dependencies():
            return False
        
        # Traitement
        if target_store:
            success = self.process_all_stores(target_store=target_store)
        else:
            limit = test_limit if test_mode else None
            success = self.process_all_stores(limit)
        
        # R√©sum√© final
        self.print_final_summary()
        
        return success and self.stats['successful_analyses'] > 0
    
    def print_final_summary(self):
        """Affiche le r√©sum√© final."""
        
        duration = (datetime.now() - self.stats['start_time']).total_seconds()
        
        logger.info("\\n" + "=" * 80)
        logger.info("üìä POLCO ANALYZER 3.0 - R√âSUM√â FINAL")
        logger.info("=" * 80)
        
        logger.info(f"‚è±Ô∏è Dur√©e totale: {duration:.1f} secondes")
        logger.info(f"üè™ Magasins trait√©s: {self.stats['total_stores']}")
        logger.info(f"‚úÖ Analyses r√©ussies: {self.stats['successful_analyses']}")
        logger.info(f"‚ùå Analyses √©chou√©es: {self.stats['failed_analyses']}")
        logger.info(f"üóÑÔ∏è Collection r√©sultats: {self.result_collection}")
        
        if self.stats['processing_details']:
            logger.info("\\nüìà D√©tails des analyses r√©ussies:")
            for store_id, details in self.stats['processing_details'].items():
                logger.info(f"   üè™ {store_id}:")
                logger.info(f"      ‚Ä¢ {details['sections_completed']} sections: {', '.join(details['sections_list'])}")
                logger.info(f"      ‚Ä¢ {details['total_output_length']:,} caract√®res g√©n√©r√©s")
                logger.info(f"      ‚Ä¢ {details['charts_generated']} graphiques")
                logger.info(f"      ‚Ä¢ {details['processing_time']:.1f}s de traitement")
        
        if self.stats['errors']:
            logger.warning(f"\\n‚ö†Ô∏è {len(self.stats['errors'])} erreurs d√©tect√©es:")
            for error in self.stats['errors']:
                logger.warning(f"   ‚Ä¢ {error}")
        
        if self.stats['successful_analyses'] > 0:
            logger.info("\\nüéâ POLCO ANALYZER 3.0 - Analyses sectorielles termin√©es avec succ√®s !")
            logger.info("üîÑ Consultez les rapports complets dans Firestore")
        else:
            logger.error("\\n‚ùå Aucune analyse compl√©t√©e avec succ√®s")


def main():
    """Point d'entr√©e principal."""
    try:
        import argparse
        parser = argparse.ArgumentParser(description='POLCO ANALYZER - Analyses sectorielles ultra-d√©taill√©es')
        parser.add_argument('--test', action='store_true', 
                          help='Mode test (traite seulement 1 magasin)')
        parser.add_argument('--limit', type=int, default=1,
                          help='Nombre de magasins √† traiter en mode test')
        parser.add_argument('--force', action='store_true',
                          help='Force la reg√©n√©ration m√™me si analyses existent')
        parser.add_argument('--store-id', type=str,
                          help='ID du magasin sp√©cifique √† traiter (ex: 42)')
        
        args = parser.parse_args()
        
        # Toujours utiliser captation (version simplifi√©e)
        analyzer = PolcoAnalyzer30("polco_magasins_captation")
        analyzer.force_regenerate = args.force
        
        # Utiliser --store-id
        target_store = getattr(args, 'store_id', None)
        success = analyzer.run(test_mode=args.test, test_limit=args.limit, target_store=target_store)
        
        return 0 if success else 1
        
    except KeyboardInterrupt:
        logger.info("\\n‚èπÔ∏è Analyse interrompue par l'utilisateur")
        return 1
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {e}")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
